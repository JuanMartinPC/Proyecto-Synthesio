{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ba3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "import collections\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c654a9a",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b8e02",
   "metadata": {},
   "source": [
    "# Fase de correccion de los datos pasandolos de STRING a DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d1997b",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c2b6b",
   "metadata": {},
   "source": [
    "## Leyendo un archivo txt que contiene nombres de archivos.csv y guardandolo en una lista con saltos de linea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e92ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivos = open('nombres_de_datos_synthesio.txt', 'r', encoding=\"UTF-8\").read().split('/n')\n",
    "archivos = [x for x in archivos[0].split('\\n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef4533",
   "metadata": {},
   "source": [
    "### En la siguente celda, leo cada archivo '.csv' del texto 'archivos' como un diccionario.\n",
    "\n",
    "### Cada diccionario contiene, a su vez, claves que contienen diccionarios que estan en formato de texto.\n",
    "\n",
    "### A continuaci√≥n, itero sobre la lista de las claves de cada nuevo diccionario y compruebo que no esten vac√≠as. Si no estan vac√≠as, procedo a corregir cada uno de los datos (cadenas de texto) que se encuentran en formato STR a DICT mediante la funci√≥n 'ast.literal_eval()'.\n",
    "\n",
    "### Una vez que los diccionarios estan en el formato correcto, los guardo como '.json', y creo un nuevo texto que contiene los nombres de cada uno de esos diccionarios.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a27e1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthesio_ep22_0_al_500.csv\n",
      "synthesio_ep22_500_al_1000.csv\n",
      "synthesio_ep22_1000_al_1500.csv\n",
      "synthesio_ep22_1500_al_2000.csv\n",
      "synthesio_ep22_2000_al_2500.csv\n"
     ]
    }
   ],
   "source": [
    "diccionario = {}\n",
    "\n",
    "for archivo in archivos:\n",
    "    diccio = dict(pd.read_csv(archivo))\n",
    "    \n",
    "    for x in list(diccio.keys()):\n",
    "        if len(diccio[x]) > 0:\n",
    "            for dato in range(len(diccio[x])):\n",
    "                correccion = ast.literal_eval(diccio[x][dato])\n",
    "                    \n",
    "                diccionario[x] = correccion\n",
    "                        \n",
    "    \n",
    "    #------------ GUARDAR\n",
    "    nombre = f'{archivo}_arreglado.json'\n",
    "    with open(nombre, 'w', encoding=\"UTF-8\") as fp:\n",
    "        json.dump(diccionario, fp)\n",
    "        \n",
    "    print(archivo)\n",
    "    diccionario = {}\n",
    "\n",
    "arreglados = open(\"arreglados_json.txt\", \"w\")\n",
    "for i in archivos:\n",
    "    if i != archivos[-1]:\n",
    "        arreglados.write(f'{i}_arreglado.json' + '\\n')\n",
    "    else:\n",
    "        arreglados.write(f'{i}_arreglado.json')\n",
    "arreglados.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae2c99d",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051d502c",
   "metadata": {},
   "source": [
    "# Fase de rellenado de nulos y claves faltantes. Creacion de un DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173336e",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6618b4c4",
   "metadata": {},
   "source": [
    "## Mediante el texto con los nombres de todos los archivos '.json', abro cada uno como un DataFrame de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "253aa49d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthesio_ep22_0_al_500.csv_arreglado.json\n",
      "synthesio_ep22_0_al_500.csv_arreglado.json\n",
      "synthesio_ep22_500_al_1000.csv_arreglado.json\n",
      "synthesio_ep22_1000_al_1500.csv_arreglado.json\n",
      "synthesio_ep22_1500_al_2000.csv_arreglado.json\n",
      "synthesio_ep22_2000_al_2500.csv_arreglado.json\n"
     ]
    }
   ],
   "source": [
    "archivos = open('arreglados_json.txt', 'r', encoding=\"UTF-8\").read().split('/n')\n",
    "archivos = [x for x in archivos[0].split('\\n')]\n",
    "print(archivos[0])\n",
    "\n",
    "with open(archivos[0], 'r') as file:\n",
    "    data_0 = json.load(file)\n",
    "\n",
    "\n",
    "n_df = pd.DataFrame()\n",
    "n_df = n_df.reindex(columns= data_0['1'][0].keys())\n",
    "\n",
    "#----------------------------------\n",
    "\n",
    "\n",
    "for archivo in archivos:\n",
    "    with open(archivo) as jsons:\n",
    "        data = json.load(jsons)\n",
    "    \n",
    "# Buscando datos faltantes y reemplazandolos por un gui√≥n\n",
    "\n",
    "    for x in list(data.keys()):\n",
    "        if len(data[x]) > 0:\n",
    "            for dato in range(len(data[x])):\n",
    "                for i in data[x][dato]:\n",
    "                    if data[x][dato][i] == None:\n",
    "                        data[x][dato][i] = str('-')\n",
    "                for y in n_df.keys():\n",
    "                    if y not in data[x][dato]:\n",
    "                        data[x][dato][y] = str('-')\n",
    "                        \n",
    "\n",
    "# Definiendo cada una de las claves del diccionario de salida    \n",
    "\n",
    "    out = {\"id\":[],\"source_id\":[],\"author_id\":[],\"site_id\":[],\"channel\":[],\"collaboration_status\":[],\"collaboration_user\":[],\n",
    "        \"content\": [], 'copyright': [], 'crawled_at': [], 'date': [], 'extra_properties': [], 'human_review_status': [],\n",
    "        'infused_at': [], 'influence': [], 'interactions': [], 'language': [], 'license': [], 'location': [], 'meta': [],\n",
    "        'metric_list': [], 'native_id': [], 'parent_id': [], 'review': [], 'root_id': [], 'sentiment': [], 'synthesio_rank': [],\n",
    "        'tags': [], 'title': [], 'type': [], 'url': [], 'noise_reducer': []}\n",
    "    \n",
    "\n",
    "# Agregando los datos correspondientes a cada clave del diccionario   \n",
    "    \n",
    "    for i in data:\n",
    "        for dato in range(len(data[i])):\n",
    "            if len(data[i]) > 0:\n",
    "                out['id'].append(data[i][dato]['id'])\n",
    "                out['source_id'].append(data[i][dato]['source_id'])\n",
    "                out['author_id'].append(data[i][dato]['author_id'])\n",
    "                out['site_id'].append(data[i][dato]['site_id'])\n",
    "                out['channel'].append(data[i][dato]['channel'])\n",
    "                out['collaboration_status'].append(data[i][dato]['collaboration_status'])\n",
    "                out['collaboration_user'].append(data[i][dato]['collaboration_user'])\n",
    "                out['content'].append(data[i][dato]['content'])\n",
    "                out['copyright'].append(data[i][dato]['copyright'])\n",
    "                out['crawled_at'].append(data[i][dato]['crawled_at'])\n",
    "                out['date'].append(data[i][dato]['date'])\n",
    "                out['extra_properties'].append(data[i][dato]['extra_properties'])\n",
    "                out['human_review_status'].append(data[i][dato]['human_review_status'])\n",
    "                out['infused_at'].append(data[i][dato]['infused_at'])\n",
    "                out['influence'].append(data[i][dato]['influence'])\n",
    "                out['interactions'].append(data[i][dato]['interactions'])\n",
    "                out['language'].append(data[i][dato]['language'])\n",
    "                out['license'].append(data[i][dato]['license'])\n",
    "                out['location'].append(data[i][dato]['location'])\n",
    "                out['meta'].append(data[i][dato]['meta'])\n",
    "                out['metric_list'].append(data[i][dato]['metric_list'])\n",
    "                out['native_id'].append(data[i][dato]['native_id'])\n",
    "                out['parent_id'].append(data[i][dato]['parent_id'])\n",
    "                out['review'].append(data[i][dato]['review'])\n",
    "                out['root_id'].append(data[i][dato]['root_id'])\n",
    "                out['sentiment'].append(data[i][dato]['sentiment'])\n",
    "                out['synthesio_rank'].append(data[i][dato]['synthesio_rank'])\n",
    "                out['tags'].append(data[i][dato]['tags'])\n",
    "                out['title'].append(data[i][dato]['title'])\n",
    "                out['type'].append(data[i][dato]['type'])\n",
    "                out['url'].append(data[i][dato]['url'])\n",
    "                out['noise_reducer'].append(data[i][dato]['noise_reducer'])\n",
    "    \n",
    "    print(archivo)\n",
    "\n",
    "# Guardando los diccionarios de salida como archivos CSV    \n",
    "\n",
    "    nombre2 = f'{archivo}_normalizado.csv'\n",
    "    csv_data = pd.DataFrame(out)\n",
    "    csv_data.to_csv(nombre2)\n",
    "\n",
    "# Escribiendo y guardando un TXT con los nombres de todos los nuevos CSV\n",
    "    \n",
    "    \n",
    "Normalizados = open(\"normalizados_csv.txt\", \"w\")\n",
    "for i in archivos:\n",
    "    if i != archivos[-1]:\n",
    "        Normalizados.write(f'{i}_normalizado.csv' + '\\n')\n",
    "    else:\n",
    "        Normalizados.write(f'{i}_normalizado.csv')\n",
    "Normalizados.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea26a060",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b1fca",
   "metadata": {},
   "source": [
    "# Fase de concatenacion de todos los DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4641e4",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72168a6c",
   "metadata": {},
   "source": [
    "## Por √∫ltimo, usando el texto con los nombres de todos los archivos CSV ya procesados y normalizados, los concateno todos en un solo DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e482562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivos = open('normalizados_csv.txt', 'r', encoding=\"UTF-8\").read().split('/n')\n",
    "archivos = [x for x in archivos[0].split('\\n')]\n",
    "\n",
    "\n",
    "csv_main = pd.read_csv(archivos[0])\n",
    "final_csv = csv_main\n",
    "\n",
    "for i in archivos:\n",
    "    csv_main = pd.read_csv(i)\n",
    "    final_csv = pd.concat([final_csv, csv_main], axis=0)\n",
    "    final_csv.drop('Unnamed: 0', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717ab958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>site_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>collaboration_status</th>\n",
       "      <th>collaboration_user</th>\n",
       "      <th>content</th>\n",
       "      <th>copyright</th>\n",
       "      <th>crawled_at</th>\n",
       "      <th>...</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>review</th>\n",
       "      <th>root_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>synthesio_rank</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>noise_reducer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>385190-186256683749</td>\n",
       "      <td>1650630-17841406056863686</td>\n",
       "      <td>1650630-17841406056863686</td>\n",
       "      <td>1650630</td>\n",
       "      <td>earned</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>üì¢El Domingo 10 de octubre ven√≠ a pasar una tar...</td>\n",
       "      <td>-</td>\n",
       "      <td>2021-09-28T04:53:48+02:00</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>{'date': '2022-01-27T11:53:32+01:00', 'status'...</td>\n",
       "      <td>186256683749</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'value': 'c3svf448hb0v1jlfs1rg', 'sentiment'...</td>\n",
       "      <td>-</td>\n",
       "      <td>post</td>\n",
       "      <td>https://www.instagram.com/p/CUWOPpuA-SE/</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>385190-186252077698</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>9942</td>\n",
       "      <td>earned</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>EL ULTIMO UNICORNIO PARTE 1 CUENTO CLASICO es ...</td>\n",
       "      <td>-</td>\n",
       "      <td>2021-09-28T01:09:33+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>{'date': '2022-01-27T11:53:32+01:00', 'status'...</td>\n",
       "      <td>-</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'value': 'c3svf448hb0v1jlfs1rg', 'sentiment'...</td>\n",
       "      <td>EL ULTIMO UNICORNIO PARTE 1</td>\n",
       "      <td>-</td>\n",
       "      <td>https://www.dailymotion.com/video/x84h46j</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>385190-186308463396</td>\n",
       "      <td>1650630-17841405500647264</td>\n",
       "      <td>1650630-17841405500647264</td>\n",
       "      <td>1650630</td>\n",
       "      <td>earned</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Elije el que m√°s te guste, nosotros te lo llev...</td>\n",
       "      <td>-</td>\n",
       "      <td>2021-09-28T20:13:21+02:00</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>{'date': '2022-01-27T11:53:33+01:00', 'status'...</td>\n",
       "      <td>186308463396</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'value': 'c3svf448hb0v1jlfs1rg', 'sentiment'...</td>\n",
       "      <td>-</td>\n",
       "      <td>post</td>\n",
       "      <td>https://www.instagram.com/p/CUXq6NlNEf0/</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>385190-186363344279</td>\n",
       "      <td>1650630-17841405443127343</td>\n",
       "      <td>1650630-17841405443127343</td>\n",
       "      <td>1650630</td>\n",
       "      <td>earned</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>‚ÄºÔ∏è√öltimos d√≠as‚ÄºÔ∏èNo te quedes sin probar el amo...</td>\n",
       "      <td>-</td>\n",
       "      <td>2021-09-29T13:56:57+02:00</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>{'date': '2022-01-27T11:53:33+01:00', 'status'...</td>\n",
       "      <td>186363344279</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'value': 'c3svf448hb0v1jlfs1rg', 'sentiment'...</td>\n",
       "      <td>-</td>\n",
       "      <td>post</td>\n",
       "      <td>https://www.instagram.com/p/CUYiN_trRjj/</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>385190-186497174014</td>\n",
       "      <td>1650630-17841402942558608</td>\n",
       "      <td>1650630-17841402942558608</td>\n",
       "      <td>1650630</td>\n",
       "      <td>earned</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>üçî üç∫ Burgers + Cerveza üçî üç∫ ‚†Ä Te esperamos en nu...</td>\n",
       "      <td>-</td>\n",
       "      <td>2021-10-01T06:06:00+02:00</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>{'date': '2022-01-27T11:53:33+01:00', 'status'...</td>\n",
       "      <td>186497174014</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'value': 'c3svf448hb0v1jlfs1rg', 'sentiment'...</td>\n",
       "      <td>-</td>\n",
       "      <td>post</td>\n",
       "      <td>https://www.instagram.com/p/CUX3f_JsiNi/</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                  source_id                  author_id  \\\n",
       "0  385190-186256683749  1650630-17841406056863686  1650630-17841406056863686   \n",
       "1  385190-186252077698                          -                          -   \n",
       "2  385190-186308463396  1650630-17841405500647264  1650630-17841405500647264   \n",
       "3  385190-186363344279  1650630-17841405443127343  1650630-17841405443127343   \n",
       "4  385190-186497174014  1650630-17841402942558608  1650630-17841402942558608   \n",
       "\n",
       "   site_id channel collaboration_status collaboration_user  \\\n",
       "0  1650630  earned                    -                  -   \n",
       "1     9942  earned                    -                  -   \n",
       "2  1650630  earned                    -                  -   \n",
       "3  1650630  earned                    -                  -   \n",
       "4  1650630  earned                    -                  -   \n",
       "\n",
       "                                             content copyright  \\\n",
       "0  üì¢El Domingo 10 de octubre ven√≠ a pasar una tar...         -   \n",
       "1  EL ULTIMO UNICORNIO PARTE 1 CUENTO CLASICO es ...         -   \n",
       "2  Elije el que m√°s te guste, nosotros te lo llev...         -   \n",
       "3  ‚ÄºÔ∏è√öltimos d√≠as‚ÄºÔ∏èNo te quedes sin probar el amo...         -   \n",
       "4  üçî üç∫ Burgers + Cerveza üçî üç∫ ‚†Ä Te esperamos en nu...         -   \n",
       "\n",
       "                  crawled_at  ... parent_id  \\\n",
       "0  2021-09-28T04:53:48+02:00  ...         -   \n",
       "1  2021-09-28T01:09:33+00:00  ...         -   \n",
       "2  2021-09-28T20:13:21+02:00  ...         -   \n",
       "3  2021-09-29T13:56:57+02:00  ...         -   \n",
       "4  2021-10-01T06:06:00+02:00  ...         -   \n",
       "\n",
       "                                              review       root_id sentiment  \\\n",
       "0  {'date': '2022-01-27T11:53:32+01:00', 'status'...  186256683749  positive   \n",
       "1  {'date': '2022-01-27T11:53:32+01:00', 'status'...             -   neutral   \n",
       "2  {'date': '2022-01-27T11:53:33+01:00', 'status'...  186308463396   neutral   \n",
       "3  {'date': '2022-01-27T11:53:33+01:00', 'status'...  186363344279   neutral   \n",
       "4  {'date': '2022-01-27T11:53:33+01:00', 'status'...  186497174014   neutral   \n",
       "\n",
       "  synthesio_rank                                               tags  \\\n",
       "0              5  [{'value': 'c3svf448hb0v1jlfs1rg', 'sentiment'...   \n",
       "1              5  [{'value': 'c3svf448hb0v1jlfs1rg', 'sentiment'...   \n",
       "2              5  [{'value': 'c3svf448hb0v1jlfs1rg', 'sentiment'...   \n",
       "3              5  [{'value': 'c3svf448hb0v1jlfs1rg', 'sentiment'...   \n",
       "4              5  [{'value': 'c3svf448hb0v1jlfs1rg', 'sentiment'...   \n",
       "\n",
       "                         title  type  \\\n",
       "0                            -  post   \n",
       "1  EL ULTIMO UNICORNIO PARTE 1     -   \n",
       "2                            -  post   \n",
       "3                            -  post   \n",
       "4                            -  post   \n",
       "\n",
       "                                         url noise_reducer  \n",
       "0   https://www.instagram.com/p/CUWOPpuA-SE/             -  \n",
       "1  https://www.dailymotion.com/video/x84h46j             -  \n",
       "2   https://www.instagram.com/p/CUXq6NlNEf0/             -  \n",
       "3   https://www.instagram.com/p/CUYiN_trRjj/             -  \n",
       "4   https://www.instagram.com/p/CUX3f_JsiNi/             -  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28628cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "numero = archivos[0][9:15]\n",
    "a.to_csv(f'datos{numero}{len(a)}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f61ec",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd972a15",
   "metadata": {},
   "source": [
    "# SEPARACION Y OBTENCION DE DATOS HISTORICOS Y POR SABOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365eee08",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54381c3e",
   "metadata": {},
   "source": [
    "## Esta parte del codigo no est√° automatizada y hay que abrir manualmente cada uno de los archivos \"finales\"que se obtienen de la concatenaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161136c6",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a537f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hora_actual():\n",
    "    return datetime.now().strftime('%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0468534",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_csv = pd.read_csv('<NOMBRE DEL ARCHIVO>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivos = open('Ingredients_2.txt', 'r', encoding=\"UTF-8\").read().split('/n')\n",
    "archivos = [x for x in archivos[0].split('\\n')]\n",
    "\n",
    "\n",
    "a = main_csv[['date', 'content']]\n",
    "b = {'id': [], 'sabor': []}\n",
    "\n",
    "\n",
    "print('Hora de inicio:', hora_actual())\n",
    "\n",
    "for i in range(len(a['content'])):\n",
    "    for sabor in archivos:\n",
    "        if sabor in a['content'][i].replace(',', '').replace('  ', '').replace('.', '').split():\n",
    "            b['id'].append(i)\n",
    "            b['sabor'].append(sabor)\n",
    "\n",
    "\n",
    "FS = {'sabor': [],'date': []}\n",
    "\n",
    "for i in range(len(b['id'])):\n",
    "    FS['sabor'].append(b['sabor'][i])\n",
    "    FS['date'].append(a['date'].values[b['id'][i]])\n",
    "\n",
    "print('Hora de finalizacion:', hora_actual())\n",
    "\n",
    "\n",
    "pd.DataFrame(b).to_csv('Id--sabor_.csv')\n",
    "pd.DataFrame(FS).to_csv('Historico_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e5a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d162d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(FS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
